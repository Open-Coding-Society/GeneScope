{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "layout: post\n",
    "title: P2 Computing Bias (2025)\n",
    "permalink: /compbias/\n",
    "author: Avika, Gabi, Zoe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå What is Computing Bias?\n",
    "> **Bias:** An inclination or prejudice in favor of or against a person or a group of people, typically in a way that is unfair.\n",
    "\n",
    "Computing **bias** occurs when computer programs, algorithms, or systems produce results that unfairly favor or disadvantage certain groups. This bias can result from **biased data, flawed design, or unintended consequences** of programming.\n",
    "\n",
    "---\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aeBLboArW8c?si=v52DEomr5Q7mqePO\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
    "\n",
    "## üé• Example: Netflix Recommendation Bias\n",
    "Netflix provides content recommendations to users through algorithms. However, these algorithms can introduce bias in several ways:  \n",
    "\n",
    "### üîç **How Bias Can Occur:**\n",
    "- **Majority Preference Bias:**  \n",
    "  - Recommending mostly popular content, making it hard for less popular or niche content to be discovered.  \n",
    "- **Filtering Bias:**  \n",
    "  - Filtering out content that doesn‚Äôt fit a user‚Äôs perceived interests based on limited viewing history.  \n",
    "  - For example, if a user primarily watches romantic comedies, Netflix may avoid suggesting documentaries or foreign films, even if the user would enjoy them.  \n",
    "\n",
    "---\n",
    "\n",
    "# üßê How Does Computing Bias Happen?\n",
    "Computing bias can occur for various reasons, including:  \n",
    "\n",
    "### üìÇ **1. Unrepresentative or Incomplete Data:**  \n",
    "- Algorithms trained on data that **doesn't represent real-world diversity** will produce biased results.  \n",
    "\n",
    "### üìâ **2. Flawed or Biased Data:**  \n",
    "- Historical or existing prejudices reflected in the training data can lead to biased outputs.  \n",
    "\n",
    "### üìù **3. Data Collection & Labeling:**  \n",
    "- Human annotators may introduce biases due to different cultural or personal biases during the data labeling process.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **Explicit Data vs. Implicit Data**\n",
    "\n",
    "### üìù **Explicit Data**\n",
    "**Definition:** Data that the user or programmer **directly provides**.\n",
    "\n",
    "- **Example:** On Netflix, users input personal information such as **name**, **age**, and **preferences**. They can also **rate shows** or **movies**.\n",
    "\n",
    "### üîç **Implicit Data**\n",
    "**Definition:** Data that is **inferred** from the user's actions or behavior, not directly provided.\n",
    "\n",
    "- **Example:** Netflix tracks your **viewing history**, **watch time**, and **interactions** with content. This data is then used to **recommend shows and movies** that Netflix thinks you might like.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è **Implications**\n",
    "- **Implicit Data** can lead to reinforcing **bias** by suggesting content based on **past behavior**, potentially **limiting diversity** and preventing users from discovering new genres.\n",
    "- **Explicit Data** is generally more **accurate** but can still be biased if **user input is limited** or influenced by the **design of the platform**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Types of Bias\n",
    "\n",
    "> **ü§ñ Algorithmic Bias**  \n",
    "- <ins>Algorithmic bias</ins> is bias generated from a repeatable but faulty **computer system** that produces inaccurate results.\n",
    "    - Example: A hiring algorithm at Apple is trained on past employee data but the data  shows that male candidates were hired more often than female candidates. Because of this, the system would favor male candidates over female candidates because historical hiring practices were biased toward men.\n",
    "> **üìà Data Bias**\n",
    "- <ins>Data bias</ins> occurs when the data itself includes bias caused by **incomplete or erroneous information**.\n",
    "    - Example: A healthcare AI model predicts lower disease risk for certain populations. Since the AI model hasn't been introduced to other demographics it would assume that data should include patients from a specific demographic, and not consider others.\n",
    "> **üß† Cognitive Bias** \n",
    "- <ins>Cognitive bias</ins> is when the person unintentionally introduces **their own bias** in the data.\n",
    "    - Example: A researcher conducting a study on social media usage unconsciously selects data that supports their belief that too much screen time leads to lower grades. This is a form of cognitive bias called confirmation bias because the researcher is searching for information to support their beliefs.\n",
    "\n",
    "\n",
    "## Popcorn Hack üöÄ\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <style>\n",
    "        /* Style for the button */\n",
    "        .button {\n",
    "            padding: 10px 20px;\n",
    "            font-size: 16px;\n",
    "            color: white;\n",
    "            background-color: blue;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "        }\n",
    "\n",
    "        /* Container for the flip effect */\n",
    "        .flip-container {\n",
    "            perspective: 1000px;\n",
    "        }\n",
    "\n",
    "        .flipper {\n",
    "            width: 200px;\n",
    "            height: 50px;\n",
    "            transform-style: preserve-3d;\n",
    "            transition: transform 0.6s;\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "        }\n",
    "\n",
    "        /* Initially, the answer side is hidden */\n",
    "        .front, .back {\n",
    "            position: absolute;\n",
    "            backface-visibility: hidden;\n",
    "            width: 100%;\n",
    "            height: 100%;\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            align-items: center;\n",
    "            font-size: 16px;\n",
    "        }\n",
    "\n",
    "        /* Front of the card (button) */\n",
    "        .front {\n",
    "            background-color: blue;\n",
    "            color: white;\n",
    "        }\n",
    "\n",
    "        /* Back of the card (answer) */\n",
    "        .back {\n",
    "            background-color: green;\n",
    "            color: white;\n",
    "            transform: rotateY(180deg);\n",
    "        }\n",
    "\n",
    "        /* Flipped state */\n",
    "        .flipped .flipper {\n",
    "            transform: rotateY(180deg);\n",
    "        }\n",
    "    </style>\n",
    "\n",
    "## Intentional Bias vs Unintentional Bias\n",
    "\n",
    "> Intentional Bias: The deliberate introduction of prejudice or unfairness into algorithms or systems, often by individuals or organizations, to achieve a specific outcome or advantage.\n",
    "\n",
    "Example: Imagine a company using a hiring algorithm to screen job applicants.\n",
    "- Goal of the algorithm: Select the most qualified candidates based on their resumes and experience.\n",
    "- However, the people who create this algorithm might intentionally (or unknowingly) include factors that are biased toward certain groups.\n",
    "\n",
    " For example, if the algorithm is designed to prioritize resumes with certain words or experiences that are more common among a specific gender or ethnic group, it might unfairly favor candidates from that group over others.\n",
    "\n",
    " Also, if the algorithm gives extra weight to leadership positions in high-profile companies that are predominantly male or white, it may unintentionally (but intentionally by the developers) disadvantage women or people of color who have the same qualifications but worked in different environments. \n",
    "\n",
    " > Unintentional Bias: Occurs when algorithms, often trained on flawed or incomplete data, produce results that unfairly discriminate against certain groups. \n",
    "\n",
    " Example: A facial recognition software.\n",
    "- Goal of the program: Designed to identify people based on their facial features.\n",
    "- However, if the software is trained using a large dataset of photos primarily of one race, it can have trouble identifying individuals who look different.\n",
    "\n",
    "For example, if the software is trained using pictures of people but the majority of those photos are of lighter-skinned individuals, the system may have trouble accurately recognizing people with darker skin tones.\n",
    "\n",
    "This unintentional bias happens because the developers didn‚Äôt purposefully choose to exclude people with darker skin, but because the dataset they used happened to be unbalanced.\n",
    "\n",
    "As a result, the system works better for lighter-skinned people and struggles with darker-skinned people, even though the goal is to treat everyone equally.\n",
    "\n",
    "## Popcorn Hack (quite literally üçø)\n",
    "\n",
    "Imagine you're making popcorn, and you're trying to pop every kernel in the bag. But here's the catch: The bag you bought has mostly kernels that are the same size, with only a few small kernels. When you heat them up, most of the kernels pop, but the few small kernels struggle to pop or even burn. Now, let's say you have a machine that heats the kernels based on size. It works great for the big ones, but when it comes to the small kernels, the machine doesn't heat them properly, and they stay uncooked.\n",
    "\n",
    "> What kind of bias does this scenario represent?\n",
    "\n",
    "<div class=\"flip-container\" id=\"flip-container\">\n",
    "    <div class=\"flipper\" id=\"flipper\">\n",
    "        <div class=\"front\">\n",
    "            <button class=\"button\" onclick=\"flipCard()\">Reveal Answer</button>\n",
    "        </div>\n",
    "        <div class=\"back\">\n",
    "            The answer is: Unintentional Bias!\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "Imagine you're making popcorn, but this time, you have a specific preference for a certain type of kernel ‚Äî let's say you prefer the big, fluffy kernels. So, when you go to buy the popcorn, you intentionally pick a bag that looks like it has mostly large kernels. Now, when you make the popcorn, your machine will pop mostly the big kernels because that's what you've chosen, and the small kernels won't get a fair chance to pop at all.\n",
    "\n",
    "> What kind of bias does this scenario represent?\n",
    "\n",
    "<div class=\"flip-container\" id=\"flip-container\">\n",
    "    <div class=\"flipper\" id=\"flipper\">\n",
    "        <div class=\"front\">\n",
    "            <button class=\"button\" onclick=\"flipCard()\">Reveal Answer</button>\n",
    "        </div>\n",
    "        <div class=\"back\">\n",
    "            The answer is: Intentional Bias!\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "    function flipCard() {\n",
    "        const flipContainer = document.getElementById('flip-container');\n",
    "        flipContainer.classList.toggle('flipped');\n",
    "    }\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Computing Bias\n",
    "> Presentented by: Avika, Zoe, and Gabi"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
